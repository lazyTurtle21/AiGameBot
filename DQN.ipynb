{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_simpleflappy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGNAgent(nn.Module):\n",
    "    def __init__(self, num_actions=2, num_features=3, lr=1e-5, weights_file=''):\n",
    "        super(DGNAgent, self).__init__()\n",
    "        \n",
    "        device = torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "\n",
    "        net = nn.Sequential(\n",
    "#             nn.Flatten((1,num_features)),\n",
    "            nn.Linear(num_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.1),\n",
    "            nn.Linear(16, num_actions),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        ).to(device)\n",
    "        self.net = net\n",
    "\n",
    "        torch.optim.SGD(self.net.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        if weights_file and os.path.exists(weights_file):\n",
    "            self.net.load_state_dict(torch.load(weights_file))\n",
    "            self.net.eval()\n",
    "\n",
    "        print('neural network created')\n",
    "        print(self.net)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_f):\n",
    "        output = self.net(input_f)\n",
    "        return output.detach()\n",
    "    \n",
    "    def save(self, path='weights'):\n",
    "        torch.save(self.net.state_dict(), path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural network created\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.1, inplace=False)\n",
      "  (3): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "flappyAgent = DGNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "ENV_NAME = 'SimpleFlappyDistance-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "\n",
    "max_eps = 1000\n",
    "episode = 0\n",
    "steps = 0\n",
    "screen_width = 288\n",
    "screen_height = 512\n",
    "max_speed = 10.0\n",
    "\n",
    "done = False\n",
    "steps = 0\n",
    "epsilon = 0.1\n",
    "ground_y = screen_height * 0.9\n",
    "pipe_max = ground_y - screen_height * 0.1\n",
    "pipe_min = screen_height * 0.4\n",
    "discount = .9\n",
    "replay = deque()\n",
    "replay_memory = 10000\n",
    "observe = 3200 # timesteps to observe before training\n",
    "steps = 0\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "explore = 3000000 # frames over which to anneal epsilon\n",
    "loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(feats):\n",
    "    # features => [bird speed, hor. distance to next pipe, ver. distance to next pipe]\n",
    "    new_feats = [feats[1], feats[2], feats[3] - feats[0]]\n",
    "#     new_feats[0] /= max_speed\n",
    "#     new_feats[1] /= screen_width\n",
    "#     new_feats[2] /= screen_height    \n",
    "    return new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEPS: 1, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 2, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 3, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 4, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 5, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 6, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 7, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 8, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 9, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 10, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 11, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 12, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 13, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 14, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 15, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 16, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 17, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 18, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 19, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 20, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 21, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 22, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 23, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 24, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 25, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 26, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 27, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 28, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 29, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 30, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 31, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 32, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 33, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 34, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 35, EPSILON: 0.1, ACTION: 1, REWARD: 1.0, Loss: 0\n",
      "STEPS: 36, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 37, EPSILON: 0.1, ACTION: 0, REWARD: 1.0, Loss: 0\n",
      "STEPS: 38, EPSILON: 0.1, ACTION: 0, REWARD: 0, Loss: 0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "features = preprocess_features(list(observation))\n",
    "features = torch.cuda.FloatTensor(features)\n",
    "\n",
    "\n",
    "flappyAgent.net.zero_grad()\n",
    "\n",
    "while not done:  #  Game cycle\n",
    "    if random.random() <= epsilon:\n",
    "        action = random.randrange(2)\n",
    "    else:\n",
    "        q = flappyAgent.forward(features)\n",
    "        action =  np.argmax(q.cpu()).item()\n",
    "\n",
    "    if epsilon > FINAL_EPSILON and steps > observe:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / explore\n",
    "\n",
    "    observation, reward, done, info = env.step(action)  #  Make a move\n",
    "    features1 = preprocess_features(list(observation))\n",
    "    features1 = torch.cuda.FloatTensor(features1)\n",
    "\n",
    "    replay.append((features, action, reward, features1, done))\n",
    "    \n",
    "    if len(replay) > replay_memory:\n",
    "        replay.popleft()\n",
    "\n",
    "    if steps > observe:\n",
    "        # sample a minibatch of size 32 from replay memory\n",
    "        flappyAgent.net.train()\n",
    "\n",
    "        minibatch = random.sample(replay, 32)\n",
    "        f, a, r, f1, alive = zip(*minibatch)\n",
    "        f = np.concatenate(f)\n",
    "        f1 = np.concatenate(f1)\n",
    "        targets = model.forward(f)\n",
    "        targets[range(32), a] = r + discount*np.max(model.forward(f1), axis=1)*(not done)\n",
    "#         loss += model.train_on_batch(f, targets)\n",
    "#         for i in range(32):\n",
    "        \n",
    "            ## TREBA LOSS\n",
    "\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = net(images.float())\n",
    "#         loss = criterion(output.float(), labels.long())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('TRAIN - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        print(targets)\n",
    "\n",
    "\n",
    "    steps += 1    \n",
    "    features = features1\n",
    "    \n",
    "    print(\"STEPS: \"+ str(steps) + \", EPSILON: \" + str(epsilon) + \", ACTION: \" + str(action) + \", REWARD: \" + str(reward) + \", Loss: \" + str(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
